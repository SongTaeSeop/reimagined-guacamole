{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of total stopwords: 179\n",
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n",
      "you've\n",
      "you'll\n",
      "you'd\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "she's\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "it's\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "that'll\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "don't\n",
      "should\n",
      "should've\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "aren't\n",
      "couldn\n",
      "couldn't\n",
      "didn\n",
      "didn't\n",
      "doesn\n",
      "doesn't\n",
      "hadn\n",
      "hadn't\n",
      "hasn\n",
      "hasn't\n",
      "haven\n",
      "haven't\n",
      "isn\n",
      "isn't\n",
      "ma\n",
      "mightn\n",
      "mightn't\n",
      "mustn\n",
      "mustn't\n",
      "needn\n",
      "needn't\n",
      "shan\n",
      "shan't\n",
      "shouldn\n",
      "shouldn't\n",
      "wasn\n",
      "wasn't\n",
      "weren\n",
      "weren't\n",
      "won\n",
      "won't\n",
      "wouldn\n",
      "wouldn't\n",
      "['Back', 'translation', 'is', 'a', 'quality', 'assurance', 'technique', 'that', 'can', 'add', 'clearity', 'to', 'and', 'control', 'over', 'your', 'translated', 'content', '!'] \n",
      "\n",
      "['Back', 'translation', 'quality', 'assurance', 'technique', 'add', 'clearity', 'control', 'translated', 'content', '!']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "print(f\"The number of total stopwords: {len(english_stopwords)}\")\n",
    "print(*english_stopwords, sep='\\n')\n",
    "\n",
    "#%% 원하는 불용어만 제거\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "eng_sample = \"Back translation is a quality assurance technique that can add clearity to and control over your translated content!\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = word_tokenize(eng_sample)\n",
    "\n",
    "cleaned_tokens = []\n",
    "\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        cleaned_tokens.append(w)\n",
    "\n",
    "print(tokens, '\\n')\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한국어', '전처리', '작업에서는', '주로', '불용어', '단어로', '조사', ',', '접속사', '등을', '사용합니다', '.'] \n",
      "\n",
      "['한국어', '전처리', '작업에서는', '주로', '불용어', '단어로', '조사', '접속사', '사용합니다']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "kor_sample = \"한국어 전처리 작업에서는 주로 불용어 단어로 조사, 접속사 등을 사용합니다.\"\n",
    "stop_words = [\"에서는\", \"등을\", \"합니다\", \".\", \",\"]\n",
    "\n",
    "tokens = word_tokenize(kor_sample)\n",
    "\n",
    "cleaned_tokens = []\n",
    "\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        cleaned_tokens.append(w)\n",
    "\n",
    "print(tokens, '\\n')\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True False "
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile('^[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$')\n",
    "\n",
    "emails = ['python@mail.example.com',\n",
    "          'testtest@naver.com',\n",
    "          'lululala@n-aver.com',\n",
    "          '@example.com'\n",
    "          ]\n",
    "\n",
    "for email in emails:\n",
    "    print(p.match(email) != None, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 영상취재 홍길동(hongkildong@chosun.com) 제주방송 임꺽정 홍길동(hongkildong@chosun.com)\n",
      "1 중요한 것은       꺾이지     않는 마음카타르 월드컵유행어로 이루어진    문서입니다.이 줄은 실제 뉴스에 포함되지 않은 임시 데이터임을 알립니다…\n",
      "2 Copyright ⓒ JIBS. All rights reserved. 무단 전재 및 재배포 금지.\n",
      "3 이 줄은 실제 뉴스에 포함되지 않은 임시 데이터임을 알립니다…\n",
      "4 #주가 #부동산 #폭락 #환률 #급상승 #중꺾마\n"
     ]
    }
   ],
   "source": [
    "# HTML 태그 제거\n",
    "def delete_html_tag(context):\n",
    "  \"\"\"\n",
    "  ex. <h1>뉴스 제목</h1> -> 뉴스 제목\n",
    "  \"\"\"\n",
    "  preprcessed_text = []\n",
    "\n",
    "  for text in context:\n",
    "      text = re.sub(r\"<[^>]+>\\s+(?=<)|<[^>]+>\", \"\", text).strip()\n",
    "      if text:\n",
    "          preprcessed_text.append(text)\n",
    "  return preprcessed_text\n",
    "\n",
    "text = [\n",
    "  \"영상취재 홍길동(hongkildong@chosun.com) 제주방송 임꺽정 홍길동(hongkildong@chosun.com)\",\n",
    "  \"<h1>중요한 것은       꺾이지     않는 마음</h1> <h3>카타르 월드컵</h3> <b>유행어</b>로 이루어진    문서입니다.\"\n",
    "  \"<br>이 줄은 실제 뉴스에 포함되지 않은 임시 데이터임을 알립니다…<br>\",\n",
    "  \"Copyright ⓒ JIBS. All rights reserved. 무단 전재 및 재배포 금지.\",\n",
    "  \"<이 기사는 언론사 에서 문화 섹션 으로 분류 했습 니다.>\",\n",
    "  \"<br>이 줄은 실제 뉴스에 포함되지 않은 임시 데이터임을 알립니다…<br>\",\n",
    "  \"#주가 #부동산 #폭락 #환률 #급상승 #중꺾마\"\n",
    "]\n",
    "cleaned_text = delete_html_tag(text)\n",
    "\n",
    "for i, line in enumerate(cleaned_text):\n",
    "    print(i, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "die\n",
      "watch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatier = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatier.lemmatize('dies', 'v'))\n",
    "print(lemmatier.lemmatize('watched', 'v'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
